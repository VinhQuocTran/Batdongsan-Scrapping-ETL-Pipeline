{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54866eaa-d2ca-4639-a220-766309be28d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType, LongType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import to_date, date_format\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc02b723-a42b-43bb-8301-d6494a01ef64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_empty_delta_table(table_path,schema):\n",
    "    empty_df = spark.createDataFrame([], schema=schema)\n",
    "    empty_df.write.format(\"delta\").mode(\"ignore\").save(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f367212e-a2b9-46a5-a369-248aca27870f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeBDS():\n",
    "    def __init__(self,read_path,write_path,checkpoint_path):\n",
    "        self.read_path = read_path\n",
    "        self.write_path = write_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    def get_schema(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"Diện tích\", StringType(), True),\n",
    "            StructField(\"Mức giá\", StringType(), True),\n",
    "            StructField(\"Mặt tiền\", StringType(), True),\n",
    "            StructField(\"Đường vào\", StringType(), True),\n",
    "            StructField(\"Hướng nhà\", StringType(), True),\n",
    "            StructField(\"Hướng ban công\", StringType(), True),\n",
    "            StructField(\"Số tầng\", StringType(), True),\n",
    "            StructField(\"Số phòng ngủ\", StringType(), True),\n",
    "            StructField(\"Số toilet\", StringType(), True),\n",
    "            StructField(\"Pháp lý\", StringType(), True),\n",
    "            StructField(\"Nội thất\", StringType(), True),\n",
    "            StructField(\"Ngày đăng\", StringType(), True),\n",
    "            StructField(\"Ngày hết hạn\", StringType(), True),\n",
    "            StructField(\"Loại tin\", StringType(), True),\n",
    "            StructField(\"Mã tin\", StringType(), True),\n",
    "            StructField(\"Địa chỉ\", StringType(), True),\n",
    "            StructField(\"latitude\", FloatType(), True),\n",
    "            StructField(\"longtitude\", FloatType(), True),\n",
    "            StructField(\"url\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        return schema\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        lines = (spark.readStream\n",
    "                    .format('json')\n",
    "                    .option(\"multiline\", \"true\")\n",
    "                    # .option(\"maxFilesPerTrigger\", 1) \n",
    "                    .schema(self.get_schema())\n",
    "                    .load(f\"{self.read_path}\")\n",
    "                )\n",
    "        return lines\n",
    "\n",
    "    def standarize_column_names(self,df):\n",
    "        new_columns = [\n",
    "            \"area\",\n",
    "            \"price\",\n",
    "            \"frontage\",\n",
    "            \"alley_length_to_house\",\n",
    "            \"house_direction\",\n",
    "            \"balcony_direction\",\n",
    "            \"floor_number\",\n",
    "            \"bedroom_number\",\n",
    "            \"toilet_number\",\n",
    "            \"legal_document\",\n",
    "            \"furniture\",\n",
    "            \"uploaded_date\",\n",
    "            \"expired_date\",\n",
    "            \"listing_article_tier\",\n",
    "            \"listing_id\",\n",
    "            \"full_address\",\n",
    "            \"latitude\",\n",
    "            \"longtitude\",\n",
    "            \"url\"\n",
    "        ]\n",
    "\n",
    "        # Rename the columns\n",
    "        for old_col, new_col in zip(df.columns, new_columns):\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def append_bronze_data(self,bronze_df,trigger):\n",
    "        sQuery =  ( bronze_df.writeStream\n",
    "                            .format('delta')\n",
    "                            .queryName(\"bronze-ingestion\")\n",
    "                            .option(\"checkpointLocation\",f\"{self.checkpoint_path}/bronze_table_checkpoint\")\n",
    "                            .outputMode(\"append\")\n",
    "        )\n",
    "\n",
    "        if(trigger=='batch'):\n",
    "            return ( sQuery.trigger(availableNow = True)\n",
    "                         .start(f\"{self.write_path}/bronze_delta_table\"))\n",
    "        else:\n",
    "            return ( sQuery.trigger(processingTime = trigger)\n",
    "                         .start(f\"{self.write_path}/bronze_delta_table\"))\n",
    "        \n",
    "\n",
    "    def process(self,trigger='batch'):\n",
    "        print(f\"\\nStarting Bronze Stream...\")\n",
    "        dbutils.fs.mkdirs(self.read_path)\n",
    "        # Read\n",
    "        bronze_df = self.get_raw_data()\n",
    "\n",
    "        # Transform\n",
    "        bronze_df = self.standarize_column_names(bronze_df)\n",
    "        bronze_df = bronze_df.dropDuplicates([\"listing_id\"])\n",
    "        create_empty_delta_table(f\"{self.write_path}/bronze_delta_table\",bronze_df.schema)\n",
    "\n",
    "        # Write\n",
    "        sQuery =  self.append_bronze_data(bronze_df,trigger)\n",
    "        return sQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b4eb9e-ee27-440a-81fb-c6021a570c77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(input_string):\n",
    "    translation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    cleaned_string = input_string.translate(translation_table)\n",
    "    return cleaned_string\n",
    "\n",
    "\n",
    "class SilverBDS():\n",
    "    def __init__(self,read_path,write_path,checkpoint_path):\n",
    "        self.read_path = read_path\n",
    "        self.write_path = write_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    @staticmethod\n",
    "    @udf(LongType())\n",
    "    def extract_price(price):\n",
    "        price=price.replace(',','.').strip()\n",
    "        if('tỷ' in price):\n",
    "            price=re.sub('[^0-9\\.]', '', price)\n",
    "            return round(float(price)*pow(10,9))\n",
    "        elif('triệu' in price):\n",
    "            price=re.sub('[^0-9\\.]', '', price)\n",
    "            return round(float(price)*pow(10,6))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    @udf(FloatType())\n",
    "    def extract_float(num,col_name=None):\n",
    "        if(num==None):\n",
    "            if(col_name==\"alley_length_to_house\"):\n",
    "                return 0\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            num=num.replace(',','.').strip()\n",
    "            num=re.sub('[^0-9\\.]', '', num)\n",
    "            if(len(num)==0):\n",
    "                return None\n",
    "            else:\n",
    "                return round(float(num),1)\n",
    "\n",
    "    @staticmethod\n",
    "    @udf(IntegerType())\n",
    "    def create_price_per_area(price,area):\n",
    "        if(price is None or area is None):\n",
    "            return None\n",
    "        else:\n",
    "            return round(price/area)\n",
    "\n",
    "    # Same name with remove_punctuation function over top, but this is only for UDF \n",
    "    @staticmethod\n",
    "    @udf(StringType())\n",
    "    def remove_punctuation(my_str):\n",
    "        return None if my_str==None else re.sub(r'[^\\w\\s]','',my_str.strip())\n",
    "\n",
    "    @staticmethod\n",
    "    @udf(StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"ward\", StringType(), True),\n",
    "        StructField(\"district\", StringType(), True),\n",
    "        StructField(\"province\", StringType(), True)\n",
    "    ]))\n",
    "\n",
    "    def split_full_address(self,address):\n",
    "        parts = address.split(\", \")\n",
    "        length = len(parts)\n",
    "\n",
    "        province = remove_punctuation(parts[-1]) if length >= 1 else None\n",
    "        district = remove_punctuation(parts[-2]) if length >= 2 else None\n",
    "        ward = remove_punctuation(parts[-3]) if length >= 3 else None\n",
    "        street = remove_punctuation(parts[-4]) if length >= 4 else None\n",
    "        return (street, ward, district, province) \n",
    "        \n",
    "    def read_bronze_data(self):\n",
    "        return ( spark.readStream\n",
    "                    .format('delta')\n",
    "                    .load(f\"{self.read_path}/bronze_delta_table\")\n",
    "                )\n",
    "    \n",
    "    def transform_silver(self, bronze_df):\n",
    "        date_format_str=\"dd/MM/yyyy\"     \n",
    "        silver_df = bronze_df.select(\n",
    "            col('listing_id'),\n",
    "            to_date(\"uploaded_date\", date_format_str).alias(\"uploaded_date\"),\n",
    "            to_date(\"expired_date\", date_format_str).alias(\"expired_date\"),\n",
    "            col('listing_article_tier'),\n",
    "            self.extract_float(col(\"area\")).alias('area'),\n",
    "            self.extract_price(col(\"price\")).alias('price'),\n",
    "            self.extract_float(col(\"frontage\")).alias('frontage'),\n",
    "            self.extract_float(col(\"alley_length_to_house\"),\"alley_length_to_house\").alias('alley_length_to_house'),\n",
    "            col(\"house_direction\"),\n",
    "            col(\"balcony_direction\"),\n",
    "            self.extract_float(col(\"floor_number\")).alias('floor_number').cast(IntegerType()),\n",
    "            self.extract_float(col(\"bedroom_number\")).alias('bedroom_number').cast(IntegerType()),\n",
    "            self.extract_float(col(\"toilet_number\")).alias('toilet_number').cast(IntegerType()),\n",
    "            col(\"legal_document\"),\n",
    "            self.remove_punctuation(col(\"furniture\")).alias('furniture'),\n",
    "            col('url'),\n",
    "            col('latitude'),\n",
    "            col('longtitude'),\n",
    "            col('full_address')\n",
    "        )\n",
    "        silver_df = silver_df.withColumn(\"price_per_area\", self.create_price_per_area(col(\"price\"), col(\"area\")))\n",
    "        \n",
    "        silver_df = silver_df.withColumn(\"full_address\", self.split_full_address(col(\"full_address\")))\n",
    "        \n",
    "        return silver_df\n",
    "    \n",
    "\n",
    "    def upsert(self, silver_df, batch_id):\n",
    "        delta_table_path=f\"{self.write_path}/silver_delta_table\"\n",
    "        tmp_view_name=\"silver_df_temp_view\"\n",
    "        silver_df.createOrReplaceTempView(tmp_view_name)\n",
    "        merge_statement = f\"\"\"MERGE INTO delta.`{delta_table_path}` s\n",
    "                USING {tmp_view_name} t\n",
    "                ON s.listing_id = t.listing_id\n",
    "                WHEN MATCHED THEN\n",
    "                UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "            \"\"\"\n",
    "        silver_df._jdf.sparkSession().sql(merge_statement)\n",
    "    \n",
    "    def append_silver_data(self,silver_df,trigger):\n",
    "        sQuery = (silver_df.writeStream\n",
    "                    .queryName(\"silver-processing\")\n",
    "                    .format(\"delta\")\n",
    "                    .outputMode(\"update\")\n",
    "                    .foreachBatch(self.upsert)\n",
    "                    .option(\"checkpointLocation\",f\"{self.checkpoint_path}/silver_table_checkpoint\")\n",
    "        )\n",
    "\n",
    "        if(trigger=='batch'):\n",
    "            return ( sQuery.trigger(availableNow = True)\n",
    "                         .start(f\"{self.write_path}/silver_delta_table\"))\n",
    "        else:\n",
    "            return ( sQuery.trigger(processingTime = trigger)\n",
    "                         .start(f\"{self.write_path}/silver_delta_table\"))\n",
    "\n",
    "    def process(self,trigger='batch'):\n",
    "        print(f\"\\nStarting Silver Stream...\", end='')\n",
    "\n",
    "        # Read\n",
    "        bronze_df = self.read_bronze_data()\n",
    "\n",
    "        # Transform\n",
    "        silver_df = self.transform_silver(bronze_df)\n",
    "        create_empty_delta_table(f\"{self.write_path}/silver_delta_table\",silver_df.schema)\n",
    "\n",
    "        # Write\n",
    "        sQuery = self.append_silver_data(silver_df,trigger)\n",
    "        return sQuery  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bds_streaming_pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
