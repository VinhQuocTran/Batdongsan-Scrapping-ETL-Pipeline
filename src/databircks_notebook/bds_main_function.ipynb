{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe347e93-3a35-490f-8e46-45057cf2dd73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import credentials and custom notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd5f9e4-ae81-4cc4-8aec-43e5cc569a7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/vinhquoc2049@gmail.com/get_credentials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55d6166-fb7e-4915-8a16-7f7f779397a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/vinhquoc2049@gmail.com/bds_streaming_pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ddd610-fd9f-445b-8872-99f0865a4dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Mount ADLS to Databricks Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f00a73-dcac-4719-9399-56573575c829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mount_container_adls2(container,mount_folder,sa_name,key):\n",
    "    mounted_paths = [mount.mountPoint for mount in dbutils.fs.mounts()]\n",
    "    if(mount_folder in mounted_paths):\n",
    "        print(f\"The mount folder already '{mount_folder}' exists.\")\n",
    "        return False\n",
    "    else:\n",
    "        url = f\"wasbs://{container}@{sa_name}.blob.core.windows.net/\"\n",
    "        conf = {\n",
    "            \"fs.azure.account.key.{0}.blob.core.windows.net\".format(sa_name): account_key\n",
    "        }\n",
    "        # Mount the Azure Data Lake Storage Gen2\n",
    "        dbutils.fs.mount(\n",
    "            source=url,\n",
    "            mount_point=mount_folder,\n",
    "            extra_configs=conf\n",
    "        )\n",
    "        print(f\"Mount container {container} in {sa_name} into Databrick successfully\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb54b9d-d3fd-4ceb-8c72-ac280b397a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mount folder already '/mnt/bronze' exists.\nThe mount folder already '/mnt/silver' exists.\nOut[43]: False"
     ]
    }
   ],
   "source": [
    "# Replace the placeholders with your actual values\n",
    "sa_name = spark.conf.get(\"sa_name\")\n",
    "connection_string = spark.conf.get(\"connection_string\")\n",
    "key = spark.conf.get(\"key\")\n",
    "\n",
    "container = \"bronze\"\n",
    "mount_folder = \"/mnt/bronze\"\n",
    "mount_container_adls2(container,mount_folder,sa_name,key)\n",
    "\n",
    "container = \"silver\"\n",
    "mount_folder = \"/mnt/silver\"\n",
    "mount_container_adls2(container,mount_folder,sa_name,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e307144-f71e-46cb-8533-4e65b1c102b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Connect to ADLS and run the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc4491f-5663-4b53-816e-25e567c4938a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active streaming queries.\n\nStarting Bronze Stream...\n\nStarting Silver Stream..."
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "def stop_all_streaming_queries():\n",
    "    # Get the active streaming queries\n",
    "    active_queries = spark.streams.active\n",
    "\n",
    "    # Check if there are any active queries\n",
    "    if active_queries:\n",
    "        print(\"List of Active Streaming Queries:\")\n",
    "        for query in active_queries:\n",
    "            print(f\"Query Name: {query.name}, ID: {query.id}\")\n",
    "            query.stop()\n",
    "    else:\n",
    "        print(\"No active streaming queries.\")\n",
    "\n",
    "\n",
    "def is_delta_table(table_path):\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        return delta_table is not None\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    bronze_base_dir='/mnt/bronze'\n",
    "    bronze_read_path='/mnt/bronze/scraped_data'\n",
    "\n",
    "\n",
    "    silver_base_dir='/mnt/silver'\n",
    "    silver_read_path=bronze_base_dir\n",
    "\n",
    "\n",
    "    bronze=BronzeBDS(bronze_read_path,bronze_base_dir,bronze_base_dir)\n",
    "    silver=SilverBDS(bronze_base_dir,silver_base_dir,silver_base_dir)\n",
    "\n",
    "    # Trigger as batch pipeline, change this parameter if you want stream pipeline\n",
    "    trigger='batch'\n",
    "    bzQuery=bronze.process(trigger)\n",
    "    bzQuery.awaitTermination()\n",
    "    slQuery=silver.process(trigger)\n",
    "\n",
    "\n",
    "stop_all_streaming_queries()\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bds_main_function",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
